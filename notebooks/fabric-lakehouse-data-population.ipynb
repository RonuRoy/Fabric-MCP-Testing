{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbfd5f91",
   "metadata": {},
   "source": [
    "# Fabric Lakehouse Data Population\n",
    "\n",
    "This notebook demonstrates how to populate a Fabric Lakehouse with sample data for testing and development purposes.\n",
    "\n",
    "## Overview\n",
    "- Create sample datasets\n",
    "- Write data to lakehouse tables\n",
    "- Verify data ingestion\n",
    "- Set up basic data structures for analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f9c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"LakehouseDataPopulation\").getOrCreate()\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8bdbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample customer data\n",
    "def generate_customer_data(num_customers=10000):\n",
    "    \"\"\"Generate sample customer data\"\"\"\n",
    "    \n",
    "    # Sample data for names and locations\n",
    "    first_names = ['John', 'Jane', 'Michael', 'Sarah', 'David', 'Emily', 'Robert', 'Jessica', 'William', 'Ashley']\n",
    "    last_names = ['Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia', 'Miller', 'Davis', 'Rodriguez', 'Martinez']\n",
    "    cities = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia', 'San Antonio', 'San Diego', 'Dallas', 'San Jose']\n",
    "    states = ['NY', 'CA', 'IL', 'TX', 'AZ', 'PA', 'TX', 'CA', 'TX', 'CA']\n",
    "    \n",
    "    customers = []\n",
    "    for i in range(num_customers):\n",
    "        customer = {\n",
    "            'customer_id': i + 1,\n",
    "            'first_name': random.choice(first_names),\n",
    "            'last_name': random.choice(last_names),\n",
    "            'email': f\"user{i+1}@example.com\",\n",
    "            'city': random.choice(cities),\n",
    "            'state': random.choice(states),\n",
    "            'registration_date': datetime.now() - timedelta(days=random.randint(1, 365)),\n",
    "            'is_active': random.choice([True, False]),\n",
    "            'customer_value': round(random.uniform(100, 10000), 2)\n",
    "        }\n",
    "        customers.append(customer)\n",
    "    \n",
    "    return pd.DataFrame(customers)\n",
    "\n",
    "# Generate customer data\n",
    "customers_df = generate_customer_data(5000)\n",
    "print(f\"Generated {len(customers_df)} customer records\")\n",
    "print(\"\\nSample data:\")\n",
    "print(customers_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0b82c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample product data\n",
    "def generate_product_data(num_products=1000):\n",
    "    \"\"\"Generate sample product data\"\"\"\n",
    "    \n",
    "    categories = ['Electronics', 'Clothing', 'Books', 'Home & Garden', 'Sports', 'Toys', 'Health', 'Beauty']\n",
    "    brands = ['Brand A', 'Brand B', 'Brand C', 'Brand D', 'Brand E']\n",
    "    \n",
    "    products = []\n",
    "    for i in range(num_products):\n",
    "        product = {\n",
    "            'product_id': i + 1,\n",
    "            'product_name': f\"Product {i+1}\",\n",
    "            'category': random.choice(categories),\n",
    "            'brand': random.choice(brands),\n",
    "            'price': round(random.uniform(10, 500), 2),\n",
    "            'cost': round(random.uniform(5, 300), 2),\n",
    "            'stock_quantity': random.randint(0, 1000),\n",
    "            'launch_date': datetime.now() - timedelta(days=random.randint(30, 1095)),\n",
    "            'is_discontinued': random.choice([True, False]) if random.random() < 0.1 else False\n",
    "        }\n",
    "        products.append(product)\n",
    "    \n",
    "    return pd.DataFrame(products)\n",
    "\n",
    "# Generate product data\n",
    "products_df = generate_product_data(1000)\n",
    "print(f\"Generated {len(products_df)} product records\")\n",
    "print(\"\\nSample data:\")\n",
    "print(products_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fa382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample sales transaction data\n",
    "def generate_sales_data(num_transactions=50000, customers_df=None, products_df=None):\n",
    "    \"\"\"Generate sample sales transaction data\"\"\"\n",
    "    \n",
    "    if customers_df is None or products_df is None:\n",
    "        raise ValueError(\"Customer and product data required\")\n",
    "    \n",
    "    customer_ids = customers_df['customer_id'].tolist()\n",
    "    product_ids = products_df['product_id'].tolist()\n",
    "    \n",
    "    sales = []\n",
    "    for i in range(num_transactions):\n",
    "        transaction_date = datetime.now() - timedelta(days=random.randint(1, 365))\n",
    "        customer_id = random.choice(customer_ids)\n",
    "        product_id = random.choice(product_ids)\n",
    "        \n",
    "        # Get product price for this transaction\n",
    "        product_price = products_df[products_df['product_id'] == product_id]['price'].iloc[0]\n",
    "        quantity = random.randint(1, 5)\n",
    "        discount = round(random.uniform(0, 0.3), 2) if random.random() < 0.2 else 0\n",
    "        \n",
    "        sale = {\n",
    "            'transaction_id': i + 1,\n",
    "            'customer_id': customer_id,\n",
    "            'product_id': product_id,\n",
    "            'transaction_date': transaction_date,\n",
    "            'quantity': quantity,\n",
    "            'unit_price': product_price,\n",
    "            'discount_percent': discount,\n",
    "            'total_amount': round(quantity * product_price * (1 - discount), 2),\n",
    "            'payment_method': random.choice(['Credit Card', 'Debit Card', 'Cash', 'PayPal', 'Bank Transfer']),\n",
    "            'sales_channel': random.choice(['Online', 'In-Store', 'Mobile App', 'Phone'])\n",
    "        }\n",
    "        sales.append(sale)\n",
    "    \n",
    "    return pd.DataFrame(sales)\n",
    "\n",
    "# Generate sales data\n",
    "sales_df = generate_sales_data(25000, customers_df, products_df)\n",
    "print(f\"Generated {len(sales_df)} sales transaction records\")\n",
    "print(\"\\nSample data:\")\n",
    "print(sales_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f502b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data to Lakehouse tables\n",
    "def write_to_lakehouse(df, table_name, mode=\"overwrite\"):\n",
    "    \"\"\"Write DataFrame to Lakehouse table\"\"\"\n",
    "    try:\n",
    "        # Convert pandas DataFrame to Spark DataFrame\n",
    "        spark_df = spark.createDataFrame(df)\n",
    "        \n",
    "        # Write to Delta table in the lakehouse\n",
    "        spark_df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(mode) \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .saveAsTable(table_name)\n",
    "        \n",
    "        print(f\"Successfully wrote {len(df)} records to table '{table_name}'\")\n",
    "        \n",
    "        # Show table info\n",
    "        table_info = spark.sql(f\"DESCRIBE TABLE {table_name}\")\n",
    "        print(f\"\\nTable schema for '{table_name}':\")\n",
    "        table_info.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to table '{table_name}': {str(e)}\")\n",
    "\n",
    "# Write all datasets to lakehouse\n",
    "print(\"Writing customer data to lakehouse...\")\n",
    "write_to_lakehouse(customers_df, \"customers\")\n",
    "\n",
    "print(\"\\nWriting product data to lakehouse...\")\n",
    "write_to_lakehouse(products_df, \"products\")\n",
    "\n",
    "print(\"\\nWriting sales data to lakehouse...\")\n",
    "write_to_lakehouse(sales_df, \"sales_transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7df96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data ingestion and run sample queries\n",
    "print(\"=== DATA VERIFICATION ===\\n\")\n",
    "\n",
    "# Check table counts\n",
    "tables = [\"customers\", \"products\", \"sales_transactions\"]\n",
    "\n",
    "for table in tables:\n",
    "    try:\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as count FROM {table}\").collect()[0]['count']\n",
    "        print(f\"Table '{table}': {count:,} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying table '{table}': {str(e)}\")\n",
    "\n",
    "print(\"\\n=== SAMPLE QUERIES ===\\n\")\n",
    "\n",
    "# Sample query 1: Top customers by total spend\n",
    "print(\"Top 10 customers by total spend:\")\n",
    "top_customers = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.customer_id,\n",
    "        c.first_name,\n",
    "        c.last_name,\n",
    "        c.email,\n",
    "        SUM(s.total_amount) as total_spent,\n",
    "        COUNT(s.transaction_id) as transaction_count\n",
    "    FROM customers c\n",
    "    JOIN sales_transactions s ON c.customer_id = s.customer_id\n",
    "    GROUP BY c.customer_id, c.first_name, c.last_name, c.email\n",
    "    ORDER BY total_spent DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "top_customers.show()\n",
    "\n",
    "# Sample query 2: Sales by category\n",
    "print(\"\\nSales by product category:\")\n",
    "sales_by_category = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        p.category,\n",
    "        COUNT(s.transaction_id) as transaction_count,\n",
    "        SUM(s.total_amount) as total_revenue,\n",
    "        AVG(s.total_amount) as avg_transaction_value\n",
    "    FROM products p\n",
    "    JOIN sales_transactions s ON p.product_id = s.product_id\n",
    "    GROUP BY p.category\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "sales_by_category.show()\n",
    "\n",
    "# Sample query 3: Monthly sales trend\n",
    "print(\"\\nMonthly sales trend:\")\n",
    "monthly_sales = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        YEAR(transaction_date) as year,\n",
    "        MONTH(transaction_date) as month,\n",
    "        COUNT(transaction_id) as transaction_count,\n",
    "        SUM(total_amount) as total_revenue\n",
    "    FROM sales_transactions\n",
    "    GROUP BY YEAR(transaction_date), MONTH(transaction_date)\n",
    "    ORDER BY year DESC, month DESC\n",
    "    LIMIT 12\n",
    "\"\"\")\n",
    "monthly_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36086617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional time series data for analytics\n",
    "def generate_daily_metrics(start_date, end_date):\n",
    "    \"\"\"Generate daily business metrics\"\"\"\n",
    "    \n",
    "    current_date = start_date\n",
    "    metrics = []\n",
    "    \n",
    "    while current_date <= end_date:\n",
    "        # Simulate seasonal patterns and weekday effects\n",
    "        is_weekend = current_date.weekday() >= 5\n",
    "        is_holiday = current_date.month == 12 and current_date.day in [24, 25, 31]\n",
    "        \n",
    "        base_visits = 1000\n",
    "        if is_weekend:\n",
    "            base_visits *= 1.3\n",
    "        if is_holiday:\n",
    "            base_visits *= 2.0\n",
    "        \n",
    "        daily_metric = {\n",
    "            'date': current_date,\n",
    "            'website_visits': int(base_visits + random.normal(0, 100)),\n",
    "            'unique_visitors': int(base_visits * 0.7 + random.normal(0, 50)),\n",
    "            'page_views': int(base_visits * 3.2 + random.normal(0, 200)),\n",
    "            'bounce_rate': round(random.uniform(0.3, 0.7), 3),\n",
    "            'avg_session_duration_minutes': round(random.uniform(2, 8), 2),\n",
    "            'conversion_rate': round(random.uniform(0.02, 0.08), 4),\n",
    "            'ad_spend': round(random.uniform(500, 2000), 2),\n",
    "            'organic_traffic_percent': round(random.uniform(0.4, 0.8), 3)\n",
    "        }\n",
    "        metrics.append(daily_metric)\n",
    "        current_date += timedelta(days=1)\n",
    "    \n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "# Generate daily metrics for the last year\n",
    "start_date = datetime.now() - timedelta(days=365)\n",
    "end_date = datetime.now()\n",
    "\n",
    "daily_metrics_df = generate_daily_metrics(start_date, end_date)\n",
    "print(f\"Generated {len(daily_metrics_df)} daily metric records\")\n",
    "print(\"\\nSample data:\")\n",
    "print(daily_metrics_df.head())\n",
    "\n",
    "# Write to lakehouse\n",
    "print(\"\\nWriting daily metrics to lakehouse...\")\n",
    "write_to_lakehouse(daily_metrics_df, \"daily_website_metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500e2ede",
   "metadata": {},
   "source": [
    "## Data Population Summary\n",
    "\n",
    "The lakehouse has been successfully populated with the following tables:\n",
    "\n",
    "### Core Business Tables\n",
    "1. **customers** - Customer information and profiles\n",
    "2. **products** - Product catalog with pricing and inventory\n",
    "3. **sales_transactions** - Transaction records linking customers and products\n",
    "\n",
    "### Analytics Tables\n",
    "4. **daily_website_metrics** - Daily web analytics and performance metrics\n",
    "\n",
    "### Data Volumes\n",
    "- 5,000 customer records\n",
    "- 1,000 product records  \n",
    "- 25,000 sales transactions\n",
    "- 365 days of website metrics\n",
    "\n",
    "### Next Steps\n",
    "- Run analytics queries to explore the data\n",
    "- Create additional derived tables as needed\n",
    "- Set up scheduled data refreshes\n",
    "- Implement data quality monitoring\n",
    "\n",
    "The lakehouse is now ready for analytics, reporting, and machine learning workloads!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
